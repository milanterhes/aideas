import { useIdeaService } from '@/lib/idea-service-provider'
import { useQueryClient } from '@tanstack/react-query'
import { useSession } from 'next-auth/react'
import OpenAI from 'openai'
import { useEffect, useMemo, useState } from 'react'
import { toast } from 'sonner'
import { addRawIdeasMutation } from './ideas'
import { useApiKey } from '@/lib/api-key-provider'
import { LocalIdeaService } from '@/lib/ideas'
import { usePostHog } from 'posthog-js/react'

const systemPrompt = `
    You are a helpful assistant that generates ideas for the user. If they ask you for something else, you can tell them that you can only generate ideas. Use markdown to format your responses.

    After generating ideas, ask the user if they would like to save the ideas. If they say yes, you can save the ideas for them.
`

const useChat = (apiKey: string) => {
    const queryClient = useQueryClient()
    const { status: sessionStatus } = useSession()
    const ideaService = useIdeaService(state => state.service)
    const setRemote = useIdeaService(state => state.setRemote)
    const apiKeyState = useApiKey()
    const posthog = usePostHog()
    const aiClient = useMemo(() => new OpenAI({
        apiKey,
        dangerouslyAllowBrowser: true,
    }), [apiKey])

    useEffect(() => {
        if (sessionStatus === 'authenticated') {
            setRemote()
        }
    }, [sessionStatus])

    useEffect(() => {
        if (sessionStatus === 'authenticated') {
            queryClient.invalidateQueries()
        }
    }, [ideaService])

    useEffect(() => {
        apiKeyState.setApiKey(sessionStorage.getItem('apiKey') ?? '')
    }, [])

    const setIdeas = addRawIdeasMutation.useMutation(ideaService, {
        onSuccess: () => {
            queryClient.invalidateQueries()
            setMessages(prev => [...prev, {
                role: 'assistant',
                content: `I saved the ideas ${ideaService instanceof LocalIdeaService ? 'locally' : 'to the cloud'}.`,
            }])
        },
        onError: (error) => {
            console.log(error)
            toast.error('Failed to save ideas: ' + error.message)
        }
    })

    function handleToolCall(toolCall: ToolCall) {
        if (toolCall.name === 'save_ideas') {
            setIdeas.mutate(toolCall.arguments);
        }
    }

    const [isStreaming, setIsStreaming] = useState(false)
    const [streamingMessage, setStreamingMessage] = useState<string | null>(null)

    const [messages, setMessages] = useState<OpenAI.Chat.Completions.ChatCompletionMessageParam[]>([
        {
            role: 'system',
            content: systemPrompt,
        }
    ])

    const sendPrompt = async (prompt: string) => {
        try {
            setIsStreaming(true);
            posthog.capture(
                'chat_message',
                {
                    role: 'user',
                    content: prompt,
                }
            );
            const stream = await aiClient.chat.completions.create({
                model: 'gpt-4o',
                messages: [
                    ...messages,
                    {
                        role: 'user',
                        content: prompt,
                    }
                ],
                stream: true,
                tools: [{
                    type: 'function',
                    function: {
                        name: 'save_ideas',
                        description: 'Save the ideas generated by the assistant',
                        parameters: {
                            type: 'object',
                            properties: {
                                ideas: {
                                    type: 'array',
                                    items: {
                                        type: 'object',
                                        properties: {
                                            idea: {
                                                type: 'string',
                                                description: 'The idea generated by the assistant',
                                            },
                                            context: {
                                                type: 'string',
                                                description: 'A description of the context in which the idea was generated',
                                            },
                                        },
                                    },
                                },
                            },
                        },
                    }
                }]
            });

            setMessages(prev => [...prev, {
                role: "user",
                content: prompt,
            }]);

            let content: null | string = null;
            let toolCall: ToolCall | null = null;

            for await (const chunk of stream) {
                const { nextContent, nextToolsCalls } = handleChunk({ chunk, content: content, toolCall });
                if (nextContent) {
                    content = nextContent;
                    setStreamingMessage(content);
                }
                toolCall = nextToolsCalls;
            }

            if (content) {
                setMessages(prev => [...prev, {
                    role: "assistant",
                    content: content,
                }]);
            }
            if (toolCall) {
                handleToolCall(toolCall);
            }
        } catch (error) {
            console.log(error);
        } finally {
            setIsStreaming(false);
            setStreamingMessage('');
        }
    };

    return { aiClient, sendPrompt, isStreaming, streamingMessage, messages }
}

type ToolCall = { arguments: string, name: string }

/**
 * Format the content of the chat message. It will return null if both the old and new content are null.
 * @param old The old content of the chat message.
 * @param newContent The new content of the chat message.
 */
function formatContent(old: string | null, newContent: string | null): string | null {
    if (old === null && newContent === null) {
        return null;
    }
    return `${old ?? ""}${newContent ?? ""}`;
}

const handleChunk = ({ chunk, content, toolCall }: { chunk: OpenAI.Chat.Completions.ChatCompletionChunk, content: string | null, toolCall: ToolCall | null }) => {
    const nextContent = formatContent(content, chunk.choices[0].delta?.content ?? null);

    const nextToolsCalls: ToolCall = toolCall ? { ...toolCall } : { arguments: '', name: '' };

    if (chunk.choices[0].delta?.tool_calls) {
        chunk.choices[0].delta.tool_calls.forEach((toolCall) => {
            if (toolCall.function?.name) {
                nextToolsCalls.name = toolCall.function.name;
            }
            if (toolCall.function?.arguments) {
                nextToolsCalls.arguments += toolCall.function.arguments ?? '';
            }
        });
    }

    return { nextContent, nextToolsCalls: nextToolsCalls };
}

export default useChat